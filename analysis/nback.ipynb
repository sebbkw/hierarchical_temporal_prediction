{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pingouin as pg\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "# Import custom modules\n",
    "sys.path.append(\"../\")\n",
    "from models.network_hierarchical_recurrent import NetworkHierarchicalRecurrent\n",
    "from plotting_functions import *\n",
    "\n",
    "MODEL_PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = np.load('./mnist/raw_images.npy')\n",
    "\n",
    "for i in raw_images[:5]:\n",
    "    i = (i-np.mean(i))/np.std(i)\n",
    "    vmax = np.max(np.abs(i))\n",
    "    plt.imshow(i, vmax=vmax, vmin=-vmax, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "noise = np.random.normal(size=(5, 32, 32))\n",
    "for i in noise:\n",
    "    vmax = np.max(np.abs(i))\n",
    "    plt.imshow(i, vmax=vmax, vmin=-vmax, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model, hyperparameters, loss_history = NetworkHierarchicalRecurrent.load(\n",
    "    model_path=MODEL_PATH,\n",
    "    device='cpu',\n",
    "    plot_loss_history=False\n",
    ")\n",
    "\n",
    "model_no_fb, hyperparameters, loss_history = NetworkHierarchicalRecurrent.load(\n",
    "    model_path=MODEL_PATH,\n",
    "    device='cpu',\n",
    "    plot_loss_history=False\n",
    ")\n",
    "weights = model.rnn.weight_hh_l0.detach().cpu().numpy().copy()\n",
    "weights[0:800, 800:1600] = 0\n",
    "weights[800:1600, 1600:2400] = 0\n",
    "model_no_fb.rnn.weight_hh_l0 = torch.nn.Parameter(torch.Tensor(weights).to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store running (i.e., per minibatch) loss, to be average per epoch\n",
    "def append_running_loss (running_loss_history, loss, loss_components):\n",
    "    running_loss_history[\"i\"] += 1\n",
    "    running_loss_history[\"loss\"] += loss.detach().cpu().numpy()\n",
    "\n",
    "    for k, v in loss_components.items():\n",
    "        try:\n",
    "            v = v.detach().cpu().numpy()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if k in running_loss_history:\n",
    "            running_loss_history[k] += v\n",
    "        else:\n",
    "            running_loss_history[k] = v\n",
    "\n",
    "# Function to append averaged loss values\n",
    "def append_epoch_loss (epoch_loss, running_loss_history, epoch):\n",
    "    i = running_loss_history[\"i\"]\n",
    "    \n",
    "    epoch_loss['epochs'].append(epoch)\n",
    "\n",
    "    for key in running_loss_history.keys():\n",
    "        if key != 'i':\n",
    "            if not key in epoch_loss:\n",
    "                epoch_loss[key] = []\n",
    "            epoch_loss[key].append(running_loss_history[key] / i)\n",
    "\n",
    "class MNISTClassifier (nn.Module):\n",
    "    def __init__ (self, model, n_back):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "\n",
    "        self.device      = 'cpu'\n",
    "        self.n_back      = n_back\n",
    "        self.x_          = np.load('./mnist/res_images.npy')\n",
    "        self.y_          = np.load('./mnist/labels.npy')\n",
    "\n",
    "        self.model       = model\n",
    "        \n",
    "        self.fc_in_size  = 2400\n",
    "        self.fc_out_size = 10\n",
    "\n",
    "        self.fc0         = nn.Linear(\n",
    "            in_features=self.fc_in_size,\n",
    "            out_features=self.fc_out_size\n",
    "        )\n",
    "        \n",
    "    def preprocess_data (self):\n",
    "        res_images_, labels_, n_back = self.x_, self.y_, self.n_back\n",
    "        \n",
    "        res_images_processed = np.zeros(\n",
    "            (res_images_.shape[0], 4+1+n_back, 20*40),\n",
    "            dtype=res_images_.dtype\n",
    "        )\n",
    "        res_images_processed[:, :4] = np.random.normal(size=(res_images_processed[:, :4].shape))\n",
    "        res_images_processed[:, 4] = res_images_\n",
    "        res_images_processed[:, 4+1:] = np.random.normal(size=(res_images_processed[:, 4+1:].shape))\n",
    "\n",
    "        res_images_batched = res_images_processed.reshape(-1, 100, 4+1+n_back, 20*40)\n",
    "        labels_batched     = labels_.reshape(-1, 100, 1)\n",
    "\n",
    "        res_images_batched = torch.from_numpy(res_images_batched).type(torch.FloatTensor)\n",
    "        labels_batched     = torch.from_numpy(labels_batched).type(torch.LongTensor)\n",
    "\n",
    "        train_idxs = int(res_images_batched.shape[0]*0.8)\n",
    "        \n",
    "        return (\n",
    "            res_images_batched[:train_idxs], res_images_batched[train_idxs:],\n",
    "            labels_batched[:train_idxs]    , labels_batched[train_idxs:] \n",
    "        )\n",
    "        \n",
    "    def get_model_outputs (self):\n",
    "        x_raw_train, x_raw_test, y_train, y_test = self.preprocess_data()\n",
    "        \n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test\n",
    "        \n",
    "        x_processed_train = []\n",
    "        x_processed_test  = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in x_raw_train:\n",
    "                _, h = self.model(batch)\n",
    "                x_processed_train.append(h[:, -1])\n",
    "\n",
    "            for batch in x_raw_test:\n",
    "                _, h = self.model(batch)\n",
    "                x_processed_test.append(h[:, -1])\n",
    "                \n",
    "        self.x_train = x_processed_train\n",
    "        self.x_test  = x_processed_test\n",
    "                                \n",
    "    def forward (self, x):\n",
    "        return self.fc0(x)\n",
    "        \n",
    "    def get_loss (self, y, y_hat):\n",
    "        cross_entropy = nn.functional.cross_entropy(y_hat, y[:, 0].long())\n",
    "                \n",
    "        classes = torch.argmax(y_hat, dim=1)\n",
    "        accuracy = torch.sum(classes == y[:, 0]).item()/len(y)\n",
    "                              \n",
    "        return cross_entropy, { 'accuracy': accuracy }\n",
    "    \n",
    "    def train (self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=10**-4)\n",
    "\n",
    "        train_history = {'epochs': []}\n",
    "        valid_history = {'epochs': []}\n",
    "\n",
    "        for epoch in range(1, 100+1):\n",
    "            for mode in ['validation', 'train']:\n",
    "                if mode == 'validation':\n",
    "                    x_data = self.x_test\n",
    "                    y_data = self.y_test\n",
    "                else:\n",
    "                    x_data = self.x_train\n",
    "                    y_data = self.y_train\n",
    "\n",
    "                running_loss_history = { \"i\": 0, \"loss\": 0 }\n",
    "\n",
    "                for x, y in zip(x_data, y_data):\n",
    "                    if mode == 'train':\n",
    "                        model.train()\n",
    "                        optimizer.zero_grad()\n",
    "                        y_hat = self(x)\n",
    "                        loss, loss_components = self.get_loss(y, y_hat)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else:\n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            y_hat = self(x)\n",
    "                            loss, loss_components = self.get_loss(y, y_hat)\n",
    "\n",
    "                    append_running_loss(running_loss_history, loss, loss_components)\n",
    "\n",
    "                if mode == 'train':\n",
    "                    append_epoch_loss(train_history, running_loss_history, epoch)\n",
    "                else:\n",
    "                    append_epoch_loss(valid_history, running_loss_history, epoch)\n",
    "\n",
    "            if epoch%10==0:\n",
    "                print('Epoch: {}/{}.............'.format(epoch, 100), end=' ')\n",
    "                print(\"Loss: {:.4f}.............\".format(train_history['loss'][-1]), end=' ')\n",
    "                print(\"Val accuracy: {:.4f}.............\".format(valid_history['accuracy'][-1]), end=' ')\n",
    "                print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "        return train_history, valid_history\n",
    "    \n",
    "    def get_accuracy (self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=10**-4)\n",
    "\n",
    "        x_data = torch.unsqueeze(torch.cat(self.x_test, dim=0), 1)\n",
    "        y_data = self.y_test.reshape(-1, 1, 1)\n",
    "\n",
    "        score = []\n",
    "        \n",
    "        for x, y in zip(x_data, y_data):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_hat = self(x)\n",
    "                loss, loss_components = self.get_loss(y, y_hat)\n",
    "\n",
    "            score.append(loss_components['accuracy'])\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = {\n",
    "    'full': [],\n",
    "    'no_fb': []\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'full' : model,\n",
    "    'no_fb' : model_no_fb,\n",
    "}\n",
    "\n",
    "n_back_arr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for model_key, model_i in models.items():\n",
    "    print(model_key)\n",
    "    \n",
    "    for n_back in n_back_arr:\n",
    "        print(n_back)\n",
    "        \n",
    "        MNIST_classifier = MNISTClassifier(model_i, n_back=n_back)\n",
    "        MNIST_classifier.get_model_outputs()\n",
    "        \n",
    "        _, valid_history = MNIST_classifier.train()\n",
    "        accuracy = valid_history['accuracy'][-1]\n",
    "        accuracy_data[model_key].append(accuracy*100)\n",
    "        \n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(n_back_arr, accuracy_data['full'] , label='Full model')\n",
    "plt.plot(n_back_arr, accuracy_data['no_fb'], label='No feedback')\n",
    "plt.plot([n_back_arr[0], n_back_arr[-1]], [10, 10], '--', c='black', label='Chance')\n",
    "plt.xlabel('n-back')\n",
    "plt.ylabel('MNIST accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "format_plot(fontsize=20)\n",
    "fig.set_size_inches(4, 4)\n",
    "plt.gca().get_legend().set_bbox_to_anchor((1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = np.load('./nback_data.npy', allow_pickle=True).item()\n",
    "\n",
    "n_back_arr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(n_back_arr, accuracy_data['full'] , label='Full model')\n",
    "plt.plot(n_back_arr, accuracy_data['no_fb'], label='No feedback')\n",
    "plt.plot([n_back_arr[0], n_back_arr[-1]], [10, 10], '--', c='black', label='Chance')\n",
    "plt.xlabel('n-back')\n",
    "plt.ylabel('MNIST accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "format_plot(fontsize=20)\n",
    "fig.set_size_inches(4, 4)\n",
    "plt.gca().get_legend().set_bbox_to_anchor((1, 1))\n",
    "plt.show()\n",
    "\n",
    "for p_full, p_nofb in zip(accuracy_data['full'], accuracy_data['no_fb']):\n",
    "    p_full = p_full/100\n",
    "    p_nofb = p_nofb/100\n",
    "    \n",
    "    x_full = p_full*140*100\n",
    "    x_nofb = p_nofb*140*100\n",
    "        \n",
    "    p = (x_full+x_nofb)/(2*140*100)\n",
    "        \n",
    "    z = (p_full-p_nofb)/np.sqrt((2*p**(1-p))/(140*100))\n",
    "    \n",
    "    pval = stats.norm.sf(abs(z))*2\n",
    "    \n",
    "    print(z, pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_weights (W_, pct):\n",
    "    if pct == 0:\n",
    "        return W_\n",
    "    \n",
    "    W = W_.copy()\n",
    "\n",
    "    n_weights = int(W.shape[0]*W.shape[1]*pct/100)\n",
    "        \n",
    "    pool = [*product(range(W.shape[0]), range(W.shape[1]))]\n",
    "    idxs = random.sample(pool, n_weights)\n",
    "    i_idxs, j_idxs = list(zip(*idxs))\n",
    "    \n",
    "    values = W[i_idxs, j_idxs]\n",
    "    np.random.shuffle(values)\n",
    "    W[i_idxs, j_idxs] = values\n",
    "    \n",
    "    return W\n",
    "\n",
    "trained_accuracy_data = np.load('./nback_data.npy', allow_pickle=True).item()\n",
    "random_accuracy_data  = np.load('./nback_data_randomised.npy', allow_pickle=True).item()\n",
    "\n",
    "n_back_arr      = [5]\n",
    "shuffle_pct_arr = [0, 25, 50, 75, 100]\n",
    "\n",
    "shuffled_accuracy_data   = []\n",
    "\n",
    "for shuffle_pct in shuffle_pct_arr:\n",
    "    print(shuffle_pct, '% shuffled feedback')\n",
    "\n",
    "    shuffled_model, hyperparameters, loss_history = NetworkHierarchicalRecurrent.load(\n",
    "        model_path=MODEL_PATH,\n",
    "        device='cpu',\n",
    "        plot_loss_history=False\n",
    "    )\n",
    "    weights = model.rnn.weight_hh_l0.detach().cpu().numpy().copy()\n",
    "    weights[0:800, 800:1600] = shuffle_weights(weights[0:800, 800:1600], shuffle_pct)\n",
    "    weights[800:1600, 1600:2400] = shuffle_weights(weights[800:1600, 1600:2400], shuffle_pct)\n",
    "    shuffled_model.rnn.weight_hh_l0 = torch.nn.Parameter(torch.Tensor(weights).to('cpu'))\n",
    "\n",
    "    model_accuracy = []\n",
    "    \n",
    "    for n_back in n_back_arr:\n",
    "        print('\\t', n_back)\n",
    "\n",
    "        MNIST_classifier = MNISTClassifier(shuffled_model, n_back=n_back)\n",
    "        MNIST_classifier.get_model_outputs()\n",
    "\n",
    "        _, valid_history = MNIST_classifier.train()\n",
    "        accuracy = valid_history['accuracy'][-1]\n",
    "        model_accuracy.append(accuracy*100)\n",
    "        \n",
    "    shuffled_accuracy_data += model_accuracy\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(shuffle_pct_arr, shuffled_accuracy_data, label='Shuffled feedback', c='tab:green')\n",
    "\n",
    "plt.plot([100], [trained_accuracy_data['no_fb'][5]], '*', markersize=15, label='No feedback', c='tab:orange')\n",
    "\n",
    "plt.xlabel('% shuffled feedback')\n",
    "plt.ylabel('5-back MNIST accuracy (%)')\n",
    "format_plot(fontsize=20)\n",
    "fig.set_size_inches(4, 4)\n",
    "plt.gca().get_legend().set_bbox_to_anchor((1, 1))\n",
    "plt.gca().get_legend().get_title().set(fontsize=20)\n",
    "plt.gca().get_legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_full = np.mean(shuffled_accuracy_data[-1])/100\n",
    "p_nofb = trained_accuracy_data['no_fb'][5]/100\n",
    "\n",
    "x_full = p_full*140*100\n",
    "x_nofb = p_nofb*140*100\n",
    "\n",
    "p = (x_full+x_nofb)/(2*140*100)\n",
    "\n",
    "z = (p_full-p_nofb)/np.sqrt((2*p**(1-p))/(140*100))\n",
    "\n",
    "pval = stats.norm.sf(abs(z))*2\n",
    "\n",
    "print(z, pval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allensdk]",
   "language": "python",
   "name": "conda-env-allensdk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
